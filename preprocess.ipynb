{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13046 1046 1183\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_file = open(\"dataset/implicit_train.json\",)\n",
    "test_file = open(\"dataset/implicit_test.json\",)\n",
    "dev_file = open(\"dataset/implicit_dev.json\",)\n",
    "train_data = json.load(train_file)\n",
    "test_data = json.load(test_file)\n",
    "dev_data = json.load(dev_file)\n",
    "print(len(train_data),len(test_data),len(dev_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json (r'dataset/implicit_train.json')\n",
    "df.to_csv (r'dataset/implicit_train.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!git lfs clone https://huggingface.co/roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ['however', 'while', 'yet', 'but', 'nevertheless', 'on the other hand', 'in fact', 'although', 'by comparison', 'whereas', 'by contrast', 'and', 'even though', 'still', 'though', 'rather', 'instead', 'on the contrary', 'in contrast', 'in turn', 'meanwhile', 'so', 'in comparison', 'finally', 'in particular', 'at the same time', 'because', 'then', 'nonetheless'] \n",
      "\n",
      "1 : ['because', 'as a result', 'consequently', 'inasmuch as', 'thus', 'therefore', 'so', 'since', 'as', 'accordingly', 'but', 'furthermore', 'then', 'in turn', 'indeed', 'and', 'in fact', 'for one thing', 'specifically', 'as it turns out', 'insofar as', 'hence', 'so that', 'ever since', 'in other words', 'that is', 'to this end', 'rather', 'as a consequence', 'besides', 'in particular', 'for example', 'for instance', 'in the end', 'subsequently', 'after', 'also', 'ultimately', 'finally', 'for', 'in short', 'plus'] \n",
      "\n",
      "2 : ['and', 'for example', 'although', 'in short', 'rather', 'specifically', 'also', 'in particular', 'in sum', 'nevertheless', 'for instance', 'while', 'in other words', 'furthermore', 'in addition', 'indeed', 'in fact', 'in the end', 'instead', 'because', 'that is', 'moreover', 'meanwhile', 'as', 'similarly', 'additionally', 'ultimately', 'but', 'likewise', 'however', 'for one thing', 'so', 'overall', 'or', 'thus', 'on the one hand', 'in turn', 'by comparison', 'then', 'further', 'besides', 'since', 'on the whole', 'first', 'as a result', 'by contrast', 'hence', 'yet', 'later', 'when', 'in summary', 'particularly', 'finally', 'simultaneously', 'still', 'accordingly', 'consequently', \"what's more\", 'even though', 'whereas', 'therefore', 'eventually', 'incidentally', 'plus', 'at the time', 'inasmuch as', 'so far', 'after', 'separately', 'subsequently', 'at the same time'] \n",
      "\n",
      "3 : ['then', 'next', 'previously', 'subsequently', 'meanwhile', 'while', 'as', 'consequently', 'at the time', 'later', 'and', 'but', 'however', 'indeed', 'since then', 'earlier', 'simultaneously', 'first', 'when', 'soon', 'because', 'whereas', 'ever since', 'finally', 'afterwards', 'so', 'after', 'in turn', 'thereafter', 'ultimately', 'before', 'also', 'by contrast', 'at the same time', 'in fact', 'eventually', 'in the end', 'in the meantime'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyword_dict = {0:[],1:[],2:[],3:[]}\n",
    "def get_keyword(data):\n",
    "    for dict in data:\n",
    "        for index in dict[\"label\"]:\n",
    "            tmp_ls = []\n",
    "            if not dict[\"conn\"] in keyword_dict[index]:\n",
    "                tmp_ls = keyword_dict[index]\n",
    "                tmp_ls.append(dict[\"conn\"])\n",
    "get_keyword(train_data)\n",
    "for k,v in keyword_dict.items():\n",
    "    print(k,\":\",v,\"\\n\")\n",
    "\n",
    "'''\n",
    "0 : comparison\n",
    "1 : contingent\n",
    "2 : expansion\n",
    "3 : temporal\n",
    "'''\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "class Dataset(torchdata.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.d1, self.d2, self.d3, self.d4 = torch.load(data_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.d1[index], self.d2[index], self.d3[index], self.d4[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.d4)\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, use_cuda, conf, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = conf.batch_size\n",
    "        kwargs = {'batch_size':batch_size, 'shuffle':conf.shuffle, 'drop_last':False}\n",
    "        if use_cuda:\n",
    "            kwargs['pin_memory'] = True\n",
    "        if conf.corpus_splitting == 1:\n",
    "            pre = './data/processed/lin/'\n",
    "        elif conf.corpus_splitting == 2:\n",
    "            pre = './data/processed/ji/'\n",
    "        elif conf.corpus_splitting == 3:\n",
    "            pre = './data/processed/l/'\n",
    "        train_data = Dataset(pre+'train.pkl')\n",
    "        dev_data = Dataset(pre+'dev.pkl')\n",
    "        test_data = Dataset(pre+'test.pkl')\n",
    "        self.train_size = len(train_data)\n",
    "        self.dev_size = len(dev_data)\n",
    "        self.test_size = len(test_data)\n",
    "        self.train_loader = torchdata.DataLoader(train_data, **kwargs)\n",
    "        self.dev_loader = torchdata.DataLoader(dev_data, **kwargs)\n",
    "        self.test_loader = torchdata.DataLoader(test_data, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self, classnum=11, splitting=2):\n",
    "        self.i2sense = [\n",
    "            'Temporal.Asynchronous', 'Temporal.Synchrony', 'Contingency.Cause',\n",
    "            'Contingency.Pragmatic cause', 'Comparison.Contrast', 'Comparison.Concession',\n",
    "            'Expansion.Conjunction', 'Expansion.Instantiation', 'Expansion.Restatement',\n",
    "            'Expansion.Alternative','Expansion.List'\n",
    "        ]\n",
    "        self.sense2i = {\n",
    "            'Temporal.Asynchronous':0, 'Temporal.Synchrony':1, 'Contingency.Cause':2,\n",
    "            'Contingency.Pragmatic cause':3, 'Comparison.Contrast':4, 'Comparison.Concession':5,\n",
    "            'Expansion.Conjunction':6, 'Expansion.Instantiation':7, 'Expansion.Restatement':8,\n",
    "            'Expansion.Alternative':9,'Expansion.List':10\n",
    "        }\n",
    "        self.i2senseclass = ['Temporal', 'Contingency', 'Comparison', 'Expansion']\n",
    "        self.senseclass2i = {'Temporal':0, 'Contingency':1, 'Comparison':2, 'Expansion':3}\n",
    "\n",
    "        self.four_or_eleven = classnum         # 11, 4, 2\n",
    "        self.corpus_splitting = splitting        # 1 for Lin, 2 for Ji, 3 for 4-way and binary\n",
    "        if self.four_or_eleven == 4 or self.four_or_eleven == 2:\n",
    "            self.corpus_splitting = 3\n",
    "        self.binclass = 0         # 0, 1, 2, 3 self.senseclass2i\n",
    "\n",
    "        self.wordvec_path = '~/Projects/GoogleNews-vectors-negative300.bin.gz'\n",
    "        self.wordvec_dim = 300\n",
    "        self.max_sent_len = 100\n",
    "\n",
    "        ################################################################################\n",
    "        # attention\n",
    "        self.attn_topk = 2\n",
    "        self.attn_dropout = 0\n",
    "        \n",
    "        ###############################################################################\n",
    "        # char/sub\n",
    "        self.need_char = False\n",
    "        self.need_sub = True\n",
    "        if self.need_sub:\n",
    "            self.need_char = False\n",
    "        self.char_num = 262\n",
    "        self.char_padding_idx = 261\n",
    "        if self.need_sub:\n",
    "            if self.corpus_splitting == 1:\n",
    "                self.char_num = 982\n",
    "            elif self.corpus_splitting == 2:\n",
    "                self.char_num = 982\n",
    "            elif self.corpus_splitting == 3:\n",
    "                self.char_num = 982\n",
    "            self.char_padding_idx = 0\n",
    "        self.char_embed_dim = 50\n",
    "        self.char_enc_dim = 50\n",
    "        self.char_filter_num = 2\n",
    "        self.char_filter_dim = [2, 3]\n",
    "        self.char_dropout = 0\n",
    "        self.char_hid_dim = self.char_enc_dim * self.char_filter_num\n",
    "\n",
    "        ###############################################################################\n",
    "        # elmo\n",
    "        self.need_elmo = True\n",
    "        self.elmo_options = '~/Projects/ELMo/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n",
    "        self.elmo_weights = '~/Projects/ELMo/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "        self.elmo_dropout = 0\n",
    "        self.elmo_labmda = 0.001\n",
    "        self.elmo_dim = 300\n",
    "\n",
    "        ################################################################################\n",
    "        # CNNLayer RNNLayer\n",
    "        self.use_rnn = False\n",
    "\n",
    "        if self.corpus_splitting == 1:\n",
    "            self.embed_dropout = 0.4\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.embed_dropout = 0.4\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.embed_dropout = 0.4\n",
    "            \n",
    "        self.cnn_dim = self.wordvec_dim\n",
    "\n",
    "        if self.need_char or self.need_sub:\n",
    "            self.cnn_dim += self.char_hid_dim\n",
    "        if self.need_elmo:\n",
    "            self.cnn_dim += self.elmo_dim\n",
    "\n",
    "        if self.corpus_splitting == 1:\n",
    "            self.cnn_layer_num = 5\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.cnn_layer_num = 4\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.cnn_layer_num = 5\n",
    "\n",
    "        if self.corpus_splitting == 1:\n",
    "            self.cnn_kernal_size = [5, 5, 5, 5, 5]\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.cnn_kernal_size = [5, 5, 5, 5]\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.cnn_kernal_size = [3, 3, 3, 3, 3]\n",
    "\n",
    "        if self.corpus_splitting == 1:\n",
    "            self.cnn_dropout = 0.4\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.cnn_dropout = 0.4\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.cnn_dropout = 0.4\n",
    "\n",
    "        self.attned_dim = self.cnn_dim * self.attn_topk\n",
    "        self.pair_rep_dim = self.attned_dim * 2 * self.cnn_layer_num\n",
    "\n",
    "        ################################################################################\n",
    "        # Classifier\n",
    "        self.clf_class_num = self.four_or_eleven\n",
    "\n",
    "        if self.corpus_splitting == 1:\n",
    "            self.clf_fc_num = 0\n",
    "            self.clf_fc_dim = 2048\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.clf_fc_num = 0\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.clf_fc_num = 0\n",
    "        \n",
    "        if self.corpus_splitting == 1:\n",
    "            self.clf_dropout = 0.3\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.clf_dropout = 0.3\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.clf_dropout = 0.3\n",
    "        \n",
    "        if self.corpus_splitting == 1:\n",
    "            self.conn_num = 94\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.conn_num = 92\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.conn_num = 93\n",
    "        \n",
    "        ################################################################################\n",
    "        self.seed = 666\n",
    "        self.batch_size = 128\n",
    "        self.shuffle = True\n",
    "\n",
    "        if self.corpus_splitting == 1:\n",
    "            self.lr = 0.001\n",
    "        elif self.corpus_splitting == 2:\n",
    "            self.lr = 0.001\n",
    "        elif self.corpus_splitting == 3:\n",
    "            self.lr = 0.001\n",
    "        \n",
    "        self.l2_penalty = 0\n",
    "        self.grad_clip = 1\n",
    "        self.epochs = 10000\n",
    "\n",
    "        self.is_mttrain = True\n",
    "        self.lambda1 = 1\n",
    "\n",
    "        self.logdir = './res/' + datetime.now().strftime('%B%d-%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/processed/ji/we.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2708\\3722206474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2708\\3722206474.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0musecuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m     \u001b[0mwe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/processed/ji/we.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m     \u001b[0mchar_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[0msub_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Python3.7\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Python3.7\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Python3.7\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/processed/ji/we.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "from allennlp.modules.elmo import Elmo\n",
    "\n",
    "class Atten(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Atten, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.w = nn.Conv1d(self.conf.cnn_dim, self.conf.cnn_dim, 1)\n",
    "        self.temper = np.power(self.conf.cnn_dim, 0.5)\n",
    "        self.dropout = nn.Dropout(self.conf.attn_dropout)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, q, v):\n",
    "        q_ = self.w(q.transpose(1, 2)).transpose(1, 2)\n",
    "        attn = torch.bmm(q_, v.transpose(1, 2)) / self.temper\n",
    "        vr = torch.bmm(self.dropout(self.softmax(attn)), v)\n",
    "        qr = torch.bmm(self.dropout(self.softmax(attn.transpose(1, 2))), q)\n",
    "        vr = torch.topk(vr, k=self.conf.attn_topk, dim=1)[0]\n",
    "        vr = vr.view(vr.size(0), -1)\n",
    "        qr = torch.topk(qr, k=self.conf.attn_topk, dim=1)[0]\n",
    "        qr = qr.view(qr.size(0), -1)\n",
    "        return qr, vr, attn\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Highway, self).__init__()\n",
    "        self.highway_linear = nn.Linear(size, size)\n",
    "        self.gate_linear = nn.Linear(size, size)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        gate = F.sigmoid(self.gate_linear(input))\n",
    "        m = self.nonlinear(self.highway_linear(input))\n",
    "        return gate * m + (1 - gate) * input\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, conf):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.rnn = nn.GRU(in_dim, in_dim, num_layers=1,\n",
    "                            dropout=self.conf.cnn_dropout, bidirectional=True)\n",
    "        self.conv = nn.Conv1d(in_dim*2, in_dim, 1)\n",
    "        nn.init.xavier_uniform(self.conv.weight)\n",
    "        self.conv.bias.data.fill_(0)\n",
    "        self.dropout = nn.Dropout(self.conf.cnn_dropout)\n",
    "\n",
    "    def forward(self, input, length, hidden=None):\n",
    "        lens, indices = torch.sort(length, 0, True)\n",
    "        maxlen = lens[0]\n",
    "        outputs, hidden_t = self.rnn(pack(input[indices], lens.tolist(), batch_first=True), hidden)\n",
    "        outputs = unpack(outputs, batch_first=True)[0]\n",
    "        _, _indices = torch.sort(indices, 0)\n",
    "        outputs = outputs[_indices]\n",
    "        size = outputs.size()\n",
    "        outputs = F.pad(outputs.unsqueeze(0), (0,0,0,self.conf.max_sent_len-maxlen)).view(size[0],-1,size[2])\n",
    "        outputs = self.conv(self.dropout(outputs.transpose(1, 2))).transpose(1, 2)\n",
    "        return outputs + input\n",
    "\n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, conf, in_dim, k, res=True):\n",
    "        super(CNNLayer, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.res = res\n",
    "        self.conv = nn.Conv1d(in_dim, in_dim*2, k, stride=1, padding=k//2)\n",
    "        nn.init.xavier_uniform(self.conv.weight)\n",
    "        self.conv.bias.data.fill_(0)\n",
    "        self.dropout = nn.Dropout(self.conf.cnn_dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.dropout(input.transpose(1, 2))\n",
    "        tmp = self.conv(output)\n",
    "        if tmp.size(2) > output.size(2):\n",
    "            output = tmp[:, :, 1:]\n",
    "        else:\n",
    "            output = tmp\n",
    "        output = output.transpose(1, 2)\n",
    "        a, b = torch.chunk(output, 2, dim=2)\n",
    "        output = a * nn.functional.sigmoid(b)\n",
    "        if self.res:\n",
    "            output = output + input\n",
    "        return output\n",
    "\n",
    "class CharLayer(nn.Module):\n",
    "    def __init__(self, char_table, conf):\n",
    "        super(CharLayer, self).__init__()\n",
    "        self.conf = conf\n",
    "        lookup, length = char_table\n",
    "        self.char_embed = nn.Embedding(self.conf.char_num, self.conf.char_embed_dim, padding_idx=self.conf.char_padding_idx)\n",
    "        self.lookup = nn.Embedding(lookup.size(0), lookup.size(1))\n",
    "        self.lookup.weight.data.copy_(lookup)\n",
    "        self.lookup.weight.requires_grad = False\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(self.conf.char_filter_num):\n",
    "            self.convs.append(nn.Conv1d(\n",
    "                self.conf.char_embed_dim, self.conf.char_enc_dim, self.conf.char_filter_dim[i],\n",
    "                stride=1, padding=self.conf.char_filter_dim[i]//2\n",
    "            ))\n",
    "            nn.init.xavier_uniform(self.convs[i].weight)\n",
    "        self.nonlinear = nn.Tanh()\n",
    "        self.mask = nn.Embedding(lookup.size(0), self.conf.char_hid_dim)\n",
    "        self.mask.weight.data.fill_(1)\n",
    "        self.mask.weight.data[0].fill_(0)\n",
    "        self.mask.weight.data[1].fill_(0)\n",
    "        self.mask.weight.requires_grad = False\n",
    "        self.highway = Highway(self.conf.char_hid_dim)\n",
    "        del lookup\n",
    "        del length\n",
    "\n",
    "    def forward(self, input):\n",
    "        charseq = self.lookup(input).long().view(input.size(0)*input.size(1), -1)\n",
    "        charseq = self.char_embed(charseq).transpose(1, 2)\n",
    "        conv_out = []\n",
    "        for i in range(self.conf.char_filter_num):\n",
    "            tmp = self.nonlinear(self.convs[i](charseq))\n",
    "            if tmp.size(2) > charseq.size(2):\n",
    "                tmp = tmp[:, :, 1:]\n",
    "            tmp = torch.topk(tmp, k=1)[0]\n",
    "            conv_out.append(torch.squeeze(tmp, dim=2))\n",
    "        hid = torch.cat(conv_out, dim=1)\n",
    "        hid = self.highway(hid)\n",
    "        hid = hid.view(input.size(0), input.size(1), -1)\n",
    "        mask = self.mask(input)\n",
    "        hid = hid * mask\n",
    "        return hid\n",
    "\n",
    "class ElmoLayer(nn.Module):\n",
    "    def __init__(self, char_table, conf):\n",
    "        super(ElmoLayer, self).__init__()\n",
    "        self.conf = conf\n",
    "        lookup, length = char_table\n",
    "        self.lookup = nn.Embedding(lookup.size(0), lookup.size(1))\n",
    "        self.lookup.weight.data.copy_(lookup)\n",
    "        self.lookup.weight.requires_grad = False\n",
    "        self.elmo = Elmo(\n",
    "            os.path.expanduser(self.conf.elmo_options), os.path.expanduser(self.conf.elmo_weights),\n",
    "            num_output_representations=2, do_layer_norm=False, dropout=self.conf.embed_dropout\n",
    "        )\n",
    "        for p in self.elmo.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.w = nn.Parameter(torch.Tensor([0.5, 0.5]))\n",
    "        self.gamma = nn.Parameter(torch.ones(1))\n",
    "        self.conv = nn.Conv1d(1024, self.conf.elmo_dim, 1)\n",
    "        nn.init.xavier_uniform(self.conv.weight)\n",
    "        self.conv.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        charseq = self.lookup(input).long()\n",
    "        res = self.elmo(charseq)['elmo_representations']\n",
    "        w = F.softmax(self.w, dim=0)\n",
    "        res = self.gamma * (w[0] * res[0] + w[1] * res[1])\n",
    "        res = self.conv(res.transpose(1, 2)).transpose(1, 2)\n",
    "        return res\n",
    "\n",
    "class ArgEncoder(nn.Module):\n",
    "    def __init__(self, conf, we_tensor, char_table=None, sub_table=None, use_cuda=False, attnvis=False):\n",
    "        super(ArgEncoder, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.attnvis = attnvis\n",
    "        if self.conf.use_rnn:\n",
    "            self.usecuda = use_cuda\n",
    "        self.embed = nn.Embedding(we_tensor.size(0), we_tensor.size(1))\n",
    "        self.embed.weight.data.copy_(we_tensor)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        if self.conf.need_char:\n",
    "            self.charenc = CharLayer(char_table, self.conf)\n",
    "        if self.conf.need_sub:\n",
    "            self.charenc = CharLayer(sub_table, self.conf)\n",
    "        if self.conf.need_elmo:\n",
    "            self.elmo = ElmoLayer(char_table, self.conf)\n",
    "        self.dropout = nn.Dropout(self.conf.embed_dropout)\n",
    "\n",
    "        self.block1 = nn.ModuleList()\n",
    "        self.block2 = nn.ModuleList()\n",
    "        self.attn = Atten(self.conf)\n",
    "        for i in range(self.conf.cnn_layer_num):\n",
    "            if self.conf.use_rnn:\n",
    "                self.block1.append(RNNLayer(self.conf.cnn_dim, self.conf))\n",
    "                self.block2.append(RNNLayer(self.conf.cnn_dim, self.conf))\n",
    "            else:\n",
    "                self.block1.append(CNNLayer(self.conf, self.conf.cnn_dim, self.conf.cnn_kernal_size[i]))\n",
    "                self.block2.append(CNNLayer(self.conf, self.conf.cnn_dim, self.conf.cnn_kernal_size[i]))\n",
    "    \n",
    "    def forward(self, a1, a2):\n",
    "        if self.conf.use_rnn:\n",
    "            len1 = torch.LongTensor([torch.max(a1[i,:].data.nonzero())+1 for i in range(a1.size(0))])\n",
    "            len2 = torch.LongTensor([torch.max(a2[i,:].data.nonzero())+1 for i in range(a2.size(0))])\n",
    "            if self.usecuda:\n",
    "                len1 = len1.cuda()\n",
    "                len2 = len2.cuda()\n",
    "        arg1repr = self.embed(a1)\n",
    "        arg2repr = self.embed(a2)\n",
    "        if self.conf.need_char or self.conf.need_sub:\n",
    "            char1 = self.charenc(a1)\n",
    "            char2 = self.charenc(a2)\n",
    "            arg1repr = torch.cat((arg1repr, char1), dim=2)\n",
    "            arg2repr = torch.cat((arg2repr, char2), dim=2)\n",
    "        if self.conf.need_elmo:\n",
    "            arg1repr = torch.cat((arg1repr, self.elmo(a1)), dim=2)\n",
    "            arg2repr = torch.cat((arg2repr, self.elmo(a2)), dim=2)\n",
    "        arg1repr = self.dropout(arg1repr)\n",
    "        arg2repr = self.dropout(arg2repr)\n",
    "        outputs = []\n",
    "        attns = []\n",
    "        for i in range(self.conf.cnn_layer_num):\n",
    "            if self.conf.use_rnn:\n",
    "                arg1repr = self.block1[i](arg1repr, len1)\n",
    "                arg2repr = self.block2[i](arg2repr, len2)\n",
    "            else:\n",
    "                arg1repr = self.block1[i](arg1repr)\n",
    "                arg2repr = self.block2[i](arg2repr)\n",
    "            outputc1, outputc2, attnw = self.attn(arg1repr, arg2repr)\n",
    "            outputs.append(outputc1)\n",
    "            outputs.append(outputc2)\n",
    "            attns.append(attnw)\n",
    "        if self.attnvis:\n",
    "            return torch.cat(outputs, 1), attns\n",
    "        else:\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nclass, conf):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.dropout = nn.Dropout(self.conf.clf_dropout)\n",
    "        self.fc = nn.ModuleList()\n",
    "        if self.conf.clf_fc_num > 0:\n",
    "            self.fc.append(nn.Linear(self.conf.pair_rep_dim, self.conf.clf_fc_dim))\n",
    "            for i in range(self.conf.clf_fc_num - 1):\n",
    "                self.fc.append(nn.Linear(self.conf.clf_fc_dim, self.conf.clf_fc_dim))\n",
    "            lastfcdim = self.conf.clf_fc_dim\n",
    "        else:\n",
    "            lastfcdim = self.conf.pair_rep_dim\n",
    "        self.lastfc = nn.Linear(lastfcdim, nclass)\n",
    "        self.nonlinear = nn.Tanh()\n",
    "        self._init_weight()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for i in range(self.conf.clf_fc_num):\n",
    "            self.fc[i].bias.data.fill_(0)\n",
    "            nn.init.uniform(self.fc[i].weight, -0.01, 0.01)\n",
    "        self.lastfc.bias.data.fill_(0)\n",
    "        nn.init.uniform(self.lastfc.weight, -0.01, 0.01)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for i in range(self.conf.clf_fc_num):\n",
    "            output = self.nonlinear(self.dropout(self.fc[i](output)))\n",
    "        output = self.lastfc(self.dropout(output))\n",
    "        return output\n",
    "\n",
    "class IDRCModel(nn.Module):\n",
    "    def __init__(self, conf, we_tensor, char_table, sub_table, use_cuda):\n",
    "        super(IDRCModel, self).__init__()\n",
    "        self.enc = ArgEncoder(conf, we_tensor, char_table, sub_table, use_cuda)\n",
    "        self.clf = Classifier(conf.clf_class_num, conf)\n",
    "\n",
    "    def forward(self, arg1, arg2):\n",
    "        return self.clf(self.enc(arg1, arg2))\n",
    "\n",
    "def test():\n",
    "    conf = Config()\n",
    "    usecuda = True\n",
    "    we = torch.load('./data/processed/ji/we.pkl')\n",
    "    char_table = None\n",
    "    sub_table = None\n",
    "    if conf.need_char or conf.need_elmo:\n",
    "        char_table = torch.load('./data/processed/ji/char_table.pkl')\n",
    "    if conf.need_sub:\n",
    "        sub_table = torch.load('./data/processed/ji/sub_table.pkl')\n",
    "    model = IDRCModel(conf, we, char_table, sub_table, usecuda)\n",
    "    if usecuda:\n",
    "        model.cuda()\n",
    "    d = Data(usecuda, conf)\n",
    "    for a1, a2, sense, conn in d.train_loader:\n",
    "        if usecuda:\n",
    "            a1, a2 = a1.cuda(), a2.cuda()\n",
    "        a1, a2 = Variable(a1), Variable(a2)\n",
    "        break\n",
    "    model.eval()\n",
    "    out = model(a1, a2)\n",
    "    print(out)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a5386850cfb9fa7c80b45611d4d4fe5cb8e2c707cacd825b30bedc7efa209b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
